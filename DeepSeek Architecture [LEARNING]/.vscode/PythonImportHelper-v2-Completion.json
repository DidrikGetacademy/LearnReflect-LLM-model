[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "safe_open",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "load_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "weight_dequant",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "act_quant",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "weight_dequant",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "triton",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton",
        "description": "triton",
        "detail": "triton",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "triton",
        "description": "triton",
        "isExtraImport": true,
        "detail": "triton",
        "documentation": {}
    },
    {
        "label": "triton.language",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton.language",
        "description": "triton.language",
        "detail": "triton.language",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.convert",
        "description": "DeepSeek-V3.inference.convert",
        "peekOfCode": "def main(hf_ckpt_path, save_path, n_experts, mp):\n    \"\"\"\n    Converts and saves model checkpoint files into a specified format.\n    Args:\n        hf_ckpt_path (str): Path to the directory containing the input checkpoint files.\n        save_path (str): Path to the directory where the converted checkpoint files will be saved.\n        n_experts (int): Total number of experts in the model.\n        mp (int): Model parallelism factor.\n    Returns:\n        None",
        "detail": "DeepSeek-V3.inference.convert",
        "documentation": {}
    },
    {
        "label": "mapping",
        "kind": 5,
        "importPath": "DeepSeek-V3.inference.convert",
        "description": "DeepSeek-V3.inference.convert",
        "peekOfCode": "mapping = {\n    \"embed_tokens\": (\"embed\", 0),\n    \"input_layernorm\": (\"attn_norm\", None),\n    \"post_attention_layernorm\": (\"ffn_norm\", None),\n    \"q_proj\": (\"wq\", 0),\n    \"q_a_proj\": (\"wq_a\", None),\n    \"q_a_layernorm\": (\"q_norm\", None),\n    \"q_b_proj\": (\"wq_b\", 0),\n    \"kv_a_proj_with_mqa\": (\"wkv_a\", None),\n    \"kv_a_layernorm\": (\"kv_norm\", None),",
        "detail": "DeepSeek-V3.inference.convert",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.fp8_cast_bf16",
        "description": "DeepSeek-V3.inference.fp8_cast_bf16",
        "peekOfCode": "def main(fp8_path, bf16_path):\n    \"\"\"\n    Converts FP8 weights to BF16 and saves the converted weights.\n    This function reads FP8 weights from the specified directory, converts them to BF16,\n    and saves the converted weights to another specified directory. It also updates the\n    model index file to reflect the changes.\n    Args:\n    fp8_path (str): The path to the directory containing the FP8 weights and model index file.\n    bf16_path (str): The path to the directory where the converted BF16 weights will be saved.\n    Raises:",
        "detail": "DeepSeek-V3.inference.fp8_cast_bf16",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.generate",
        "description": "DeepSeek-V3.inference.generate",
        "peekOfCode": "def sample(logits, temperature: float = 1.0):\n    \"\"\"\n    Samples a token from the logits using temperature scaling.\n    Args:\n        logits (torch.Tensor): The logits tensor for token predictions.\n        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n    Returns:\n        torch.Tensor: The sampled token.\n    \"\"\"\n    logits = logits / max(temperature, 1e-5)",
        "detail": "DeepSeek-V3.inference.generate",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.generate",
        "description": "DeepSeek-V3.inference.generate",
        "peekOfCode": "def generate(\n    model: Transformer,\n    prompt_tokens: List[List[int]],\n    max_new_tokens: int,\n    eos_id: int,\n    temperature: float = 1.0\n) -> List[List[int]]:\n    \"\"\"\n    Generates new tokens based on the given prompt tokens using the specified model.\n    Args:",
        "detail": "DeepSeek-V3.inference.generate",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.generate",
        "description": "DeepSeek-V3.inference.generate",
        "peekOfCode": "def main(\n    ckpt_path: str,\n    config: str,\n    input_file: str = \"\",\n    interactive: bool = True,\n    max_new_tokens: int = 100,\n    temperature: float = 1.0,\n) -> None:\n    \"\"\"\n    Main function to load the model and perform interactive or batch text generation.",
        "detail": "DeepSeek-V3.inference.generate",
        "documentation": {}
    },
    {
        "label": "act_quant_kernel",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Quantizes the input tensor `x_ptr` and stores the result in `y_ptr` and the scaling factor in `s_ptr`.\n    Args:\n        x_ptr (triton.Pointer): Pointer to the input tensor.\n        y_ptr (triton.Pointer): Pointer to the output tensor where quantized values will be stored.\n        s_ptr (triton.Pointer): Pointer to the output tensor where scaling factors will be stored.\n        BLOCK_SIZE (tl.constexpr): The size of the block to be processed by each program instance.\n    Returns:\n        None",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "act_quant",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "def act_quant(x: torch.Tensor, block_size: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantizes the input tensor `x` using block-wise quantization.\n    Args:\n        x (torch.Tensor): The input tensor to be quantized. Must be contiguous and its last dimension size must be divisible by `block_size`.\n        block_size (int, optional): The size of the blocks to be used for quantization. Default is 128.\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n            - The quantized tensor with dtype `torch.float8_e4m3fn`.\n            - A tensor of scaling factors with dtype `torch.float32`.",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "weight_dequant_kernel",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "def weight_dequant_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Dequantizes weights using the provided scaling factors and stores the result.\n    Args:\n        x_ptr (tl.pointer): Pointer to the quantized weights.\n        s_ptr (tl.pointer): Pointer to the scaling factors.\n        y_ptr (tl.pointer): Pointer to the output buffer for dequantized weights.\n        M (int): Number of rows in the weight matrix.\n        N (int): Number of columns in the weight matrix.\n        BLOCK_SIZE (tl.constexpr): Size of the block for tiling.",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "weight_dequant",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "def weight_dequant(x: torch.Tensor, s: torch.Tensor, block_size: int = 128) -> torch.Tensor:\n    \"\"\"\n    Dequantizes the given weight tensor using the provided scale tensor.\n    Args:\n        x (torch.Tensor): The quantized weight tensor of shape (M, N).\n        s (torch.Tensor): The scale tensor of shape (M, N).\n        block_size (int, optional): The block size to use for dequantization. Defaults to 128.\n    Returns:\n        torch.Tensor: The dequantized weight tensor of the same shape as `x`.\n    Raises:",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm_kernel",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "def fp8_gemm_kernel(a_ptr, b_ptr, c_ptr,\n                    a_s_ptr, b_s_ptr,\n                    M, N: tl.constexpr, K: tl.constexpr,\n                    BLOCK_SIZE_M: tl.constexpr,\n                    BLOCK_SIZE_N: tl.constexpr,\n                    BLOCK_SIZE_K: tl.constexpr):\n    \"\"\"\n    Performs a matrix multiplication operation on FP8 matrices with scaling factors.\n    Args:\n        a_ptr (tl.tensor): Pointer to the first input matrix A.",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "def fp8_gemm(a: torch.Tensor, a_s: torch.Tensor, b: torch.Tensor, b_s: torch.Tensor):\n    \"\"\"\n    Perform a matrix multiplication using FP8 precision.\n    Args:\n        a (torch.Tensor): The first input matrix, must be contiguous.\n        a_s (torch.Tensor): The scaling factor for the first input matrix, must be contiguous.\n        b (torch.Tensor): The second input matrix, must be contiguous.\n        b_s (torch.Tensor): The scaling factor for the second input matrix, must be contiguous.\n    Returns:\n        torch.Tensor: The result of the matrix multiplication.",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm_configs",
        "kind": 5,
        "importPath": "DeepSeek-V3.inference.kernel",
        "description": "DeepSeek-V3.inference.kernel",
        "peekOfCode": "fp8_gemm_configs = [\n    Config({'BLOCK_SIZE_M': block_m, 'BLOCK_SIZE_N': block_n, 'BLOCK_SIZE_K': 128}, num_stages=num_stages, num_warps=8)\n    for block_m in [16, 32, 64] for block_n in [32, 64, 128] for num_stages in [3, 4, 5, 6]\n]\n@triton.autotune(configs=fp8_gemm_configs, key=['N', 'K'])\n@triton.jit\ndef fp8_gemm_kernel(a_ptr, b_ptr, c_ptr,\n                    a_s_ptr, b_s_ptr,\n                    M, N: tl.constexpr, K: tl.constexpr,\n                    BLOCK_SIZE_M: tl.constexpr,",
        "detail": "DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:\n        max_batch_size (int): Maximum batch size.\n        max_seq_len (int): Maximum sequence length.\n        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n        vocab_size (int): Vocabulary size.\n        dim (int): Model dimension.\n        inter_dim (int): Intermediate dimension for MLP layers.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "ParallelEmbedding",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class ParallelEmbedding(nn.Module):\n    \"\"\"\n    Embedding layer with parallelism support across distributed processes.\n    Args:\n        vocab_size (int): Vocabulary size.\n        dim (int): Embedding dimension.\n    \"\"\"\n    def __init__(self, vocab_size: int, dim: int):\n        super().__init__()\n        self.vocab_size = vocab_size",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Linear",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class Linear(nn.Module):\n    \"\"\"\n    Custom linear layer with support for quantized weights and optional bias.\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    dtype = torch.bfloat16",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "ColumnParallelLinear",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class ColumnParallelLinear(Linear):\n    \"\"\"\n    Linear layer with column parallelism, splitting output features across distributed processes.\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Total number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "RowParallelLinear",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class RowParallelLinear(Linear):\n    \"\"\"\n    Linear layer with row parallelism, splitting input features across distributed processes.\n    Args:\n        in_features (int): Total number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n    Args:\n        dim (int): Dimension of the input tensor.\n        eps (float): Epsilon value for numerical stability. Defaults to 1e-6.\n    \"\"\"\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.dim = dim",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "MLA",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class MLA(nn.Module):\n    \"\"\"\n    Multi-Headed Attention Layer (MLA).\n    Attributes:\n        dim (int): Dimensionality of the input features.\n        n_heads (int): Number of attention heads.\n        n_local_heads (int): Number of local attention heads for distributed systems.\n        q_lora_rank (int): Rank for low-rank query projection.\n        kv_lora_rank (int): Rank for low-rank key/value projection.\n        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Gate",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class Gate(nn.Module):\n    \"\"\"\n    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n    Attributes:\n        dim (int): Dimensionality of input features.\n        topk (int): Number of top experts activated for each input.\n        n_groups (int): Number of groups for routing.\n        topk_groups (int): Number of groups to route inputs to.\n        score_func (str): Scoring function ('softmax' or 'sigmoid').\n        route_scale (float): Scaling factor for routing weights.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Expert",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class Expert(nn.Module):\n    \"\"\"\n    Expert layer for Mixture-of-Experts (MoE) models.\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "MoE",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class MoE(nn.Module):\n    \"\"\"\n    Mixture-of-Experts (MoE) module.\n    Attributes:\n        dim (int): Dimensionality of input features.\n        n_routed_experts (int): Total number of experts in the model.\n        n_local_experts (int): Number of experts handled locally in distributed systems.\n        n_activated_experts (int): Number of experts activated for each input.\n        gate (nn.Module): Gating mechanism to route inputs to experts.\n        experts (nn.ModuleList): List of expert modules.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class Block(nn.Module):\n    \"\"\"\n    Transformer block combining attention and feed-forward layers.\n    Attributes:\n        attn (nn.Module): Attention layer (MLA).\n        ffn (nn.Module): Feed-forward network (MLP or MoE).\n        attn_norm (nn.Module): Layer normalization for attention.\n        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n    \"\"\"\n    def __init__(self, layer_id: int, args: ModelArgs):",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer model with positional embeddings, multiple layers, and output projection.\n    Attributes:\n        max_seq_len (int): Maximum sequence length for the transformer.\n        embed (nn.Module): Embedding layer for input tokens.\n        layers (torch.nn.ModuleList): List of transformer blocks.\n        norm (nn.Module): Layer normalization applied after all blocks.\n        head (nn.Module): Output projection layer mapping to vocabulary size.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "linear",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"\n    Applies a linear transformation to the incoming data: y = xA^T + b.\n    This function supports specialized implementations based on quantization\n    and tensor formats.\n    Args:\n        x (torch.Tensor): The input tensor.\n        weight (torch.Tensor): The weight tensor. It may be quantized and \n            requires dequantization for certain cases.\n        bias (Optional[torch.Tensor]): The bias tensor to be added. Default is None.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "precompute_freqs_cis",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n    \"\"\"\n    Precomputes frequency-based complex exponential values for rotary positional embeddings.\n    Args:\n        args (ModelArgs): Model arguments containing positional embedding parameters.\n    Returns:\n        torch.Tensor: Precomputed complex exponential values for positional embeddings.\n    \"\"\"\n    dim = args.qk_rope_head_dim\n    seqlen = args.max_seq_len",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "apply_rotary_emb",
        "kind": 2,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Applies rotary positional embeddings to the input tensor.\n    Args:\n        x (torch.Tensor): Input tensor with positional embeddings to be applied.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n    Returns:\n        torch.Tensor: Tensor with rotary embeddings applied.\n    \"\"\"\n    dtype = x.dtype",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "world_size",
        "kind": 5,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "world_size = 1\nrank = 0\nblock_size = 128\ngemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n@dataclass\nclass ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "rank",
        "kind": 5,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "rank = 0\nblock_size = 128\ngemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n@dataclass\nclass ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:\n        max_batch_size (int): Maximum batch size.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "block_size",
        "kind": 5,
        "importPath": "DeepSeek-V3.inference.model",
        "description": "DeepSeek-V3.inference.model",
        "peekOfCode": "block_size = 128\ngemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n@dataclass\nclass ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:\n        max_batch_size (int): Maximum batch size.\n        max_seq_len (int): Maximum sequence length.",
        "detail": "DeepSeek-V3.inference.model",
        "documentation": {}
    }
]
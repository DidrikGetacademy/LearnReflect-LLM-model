[
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Blueprint",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "CORS",
        "importPath": "flask_cors",
        "description": "flask_cors",
        "isExtraImport": true,
        "detail": "flask_cors",
        "documentation": {}
    },
    {
        "label": "chatbot",
        "importPath": "ChatbotAI.Route.Route",
        "description": "ChatbotAI.Route.Route",
        "isExtraImport": true,
        "detail": "ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "trange",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "safe_open",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "load_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "safetensors.torch",
        "description": "safetensors.torch",
        "isExtraImport": true,
        "detail": "safetensors.torch",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "weight_dequant",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "act_quant",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "weight_dequant",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm",
        "importPath": "kernel",
        "description": "kernel",
        "isExtraImport": true,
        "detail": "kernel",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2LMHeadModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2LMHeadModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainerCallback",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "DataCollatorForLanguageModeling",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Tokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2LMHeadModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "DataCollatorForLanguageModeling",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPTNeoXForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "PretrainedConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPTNeoXForCausalLM",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "triton",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton",
        "description": "triton",
        "detail": "triton",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "triton",
        "description": "triton",
        "isExtraImport": true,
        "detail": "triton",
        "documentation": {}
    },
    {
        "label": "triton.language",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "triton.language",
        "description": "triton.language",
        "detail": "triton.language",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "startup_message",
        "kind": 2,
        "importPath": "API.app",
        "description": "API.app",
        "peekOfCode": "def startup_message():\n    if not hasattr(app, 'startup_logged'):\n        logging.info(\"Flask server is starting up...\")\n        app.startup_logged = True\n@app.after_request\ndef log_requests(response):\n    logging.info(f\"{request.method} {request.path} - {response.status}\")\n    return response\nif __name__ == '__main__':\n    logging.info(\"Starting Flask server...\")",
        "detail": "API.app",
        "documentation": {}
    },
    {
        "label": "log_requests",
        "kind": 2,
        "importPath": "API.app",
        "description": "API.app",
        "peekOfCode": "def log_requests(response):\n    logging.info(f\"{request.method} {request.path} - {response.status}\")\n    return response\nif __name__ == '__main__':\n    logging.info(\"Starting Flask server...\")\n    app.run(debug=True)",
        "detail": "API.app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "API.app",
        "description": "API.app",
        "peekOfCode": "app = Flask(__name__)\nCORS(app)\n# Register blueprints\napp.register_blueprint(chatbot, url_prefix='/chatbot')\n@app.before_request\ndef startup_message():\n    if not hasattr(app, 'startup_logged'):\n        logging.info(\"Flask server is starting up...\")\n        app.startup_logged = True\n@app.after_request",
        "detail": "API.app",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.convert",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.convert",
        "peekOfCode": "def main(hf_ckpt_path, save_path, n_experts, mp):\n    \"\"\"\n    Converts and saves model checkpoint files into a specified format.\n    Args:\n        hf_ckpt_path (str): Path to the directory containing the input checkpoint files.\n        save_path (str): Path to the directory where the converted checkpoint files will be saved.\n        n_experts (int): Total number of experts in the model.\n        mp (int): Model parallelism factor.\n    Returns:\n        None",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.convert",
        "documentation": {}
    },
    {
        "label": "mapping",
        "kind": 5,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.convert",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.convert",
        "peekOfCode": "mapping = {\n    \"embed_tokens\": (\"embed\", 0),\n    \"input_layernorm\": (\"attn_norm\", None),\n    \"post_attention_layernorm\": (\"ffn_norm\", None),\n    \"q_proj\": (\"wq\", 0),\n    \"q_a_proj\": (\"wq_a\", None),\n    \"q_a_layernorm\": (\"q_norm\", None),\n    \"q_b_proj\": (\"wq_b\", 0),\n    \"kv_a_proj_with_mqa\": (\"wkv_a\", None),\n    \"kv_a_layernorm\": (\"kv_norm\", None),",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.convert",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.fp8_cast_bf16",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.fp8_cast_bf16",
        "peekOfCode": "def main(fp8_path, bf16_path):\n    \"\"\"\n    Converts FP8 weights to BF16 and saves the converted weights.\n    This function reads FP8 weights from the specified directory, converts them to BF16,\n    and saves the converted weights to another specified directory. It also updates the\n    model index file to reflect the changes.\n    Args:\n    fp8_path (str): The path to the directory containing the FP8 weights and model index file.\n    bf16_path (str): The path to the directory where the converted BF16 weights will be saved.\n    Raises:",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.fp8_cast_bf16",
        "documentation": {}
    },
    {
        "label": "sample",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "peekOfCode": "def sample(logits, temperature: float = 1.0):\n    \"\"\"\n    Samples a token from the logits using temperature scaling.\n    Args:\n        logits (torch.Tensor): The logits tensor for token predictions.\n        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n    Returns:\n        torch.Tensor: The sampled token.\n    \"\"\"\n    logits = logits / max(temperature, 1e-5)",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "documentation": {}
    },
    {
        "label": "generate",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "peekOfCode": "def generate(\n    model: Transformer,\n    prompt_tokens: List[List[int]],\n    max_new_tokens: int,\n    eos_id: int,\n    temperature: float = 1.0\n) -> List[List[int]]:\n    \"\"\"\n    Generates new tokens based on the given prompt tokens using the specified model.\n    Args:",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "peekOfCode": "def main(\n    ckpt_path: str,\n    config: str,\n    input_file: str = \"\",\n    interactive: bool = True,\n    max_new_tokens: int = 100,\n    temperature: float = 1.0,\n) -> None:\n    \"\"\"\n    Main function to load the model and perform interactive or batch text generation.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.generate",
        "documentation": {}
    },
    {
        "label": "act_quant_kernel",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Quantizes the input tensor `x_ptr` and stores the result in `y_ptr` and the scaling factor in `s_ptr`.\n    Args:\n        x_ptr (triton.Pointer): Pointer to the input tensor.\n        y_ptr (triton.Pointer): Pointer to the output tensor where quantized values will be stored.\n        s_ptr (triton.Pointer): Pointer to the output tensor where scaling factors will be stored.\n        BLOCK_SIZE (tl.constexpr): The size of the block to be processed by each program instance.\n    Returns:\n        None",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "act_quant",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "def act_quant(x: torch.Tensor, block_size: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantizes the input tensor `x` using block-wise quantization.\n    Args:\n        x (torch.Tensor): The input tensor to be quantized. Must be contiguous and its last dimension size must be divisible by `block_size`.\n        block_size (int, optional): The size of the blocks to be used for quantization. Default is 128.\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n            - The quantized tensor with dtype `torch.float8_e4m3fn`.\n            - A tensor of scaling factors with dtype `torch.float32`.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "weight_dequant_kernel",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "def weight_dequant_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Dequantizes weights using the provided scaling factors and stores the result.\n    Args:\n        x_ptr (tl.pointer): Pointer to the quantized weights.\n        s_ptr (tl.pointer): Pointer to the scaling factors.\n        y_ptr (tl.pointer): Pointer to the output buffer for dequantized weights.\n        M (int): Number of rows in the weight matrix.\n        N (int): Number of columns in the weight matrix.\n        BLOCK_SIZE (tl.constexpr): Size of the block for tiling.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "weight_dequant",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "def weight_dequant(x: torch.Tensor, s: torch.Tensor, block_size: int = 128) -> torch.Tensor:\n    \"\"\"\n    Dequantizes the given weight tensor using the provided scale tensor.\n    Args:\n        x (torch.Tensor): The quantized weight tensor of shape (M, N).\n        s (torch.Tensor): The scale tensor of shape (M, N).\n        block_size (int, optional): The block size to use for dequantization. Defaults to 128.\n    Returns:\n        torch.Tensor: The dequantized weight tensor of the same shape as `x`.\n    Raises:",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm_kernel",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "def fp8_gemm_kernel(a_ptr, b_ptr, c_ptr,\n                    a_s_ptr, b_s_ptr,\n                    M, N: tl.constexpr, K: tl.constexpr,\n                    BLOCK_SIZE_M: tl.constexpr,\n                    BLOCK_SIZE_N: tl.constexpr,\n                    BLOCK_SIZE_K: tl.constexpr):\n    \"\"\"\n    Performs a matrix multiplication operation on FP8 matrices with scaling factors.\n    Args:\n        a_ptr (tl.tensor): Pointer to the first input matrix A.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "def fp8_gemm(a: torch.Tensor, a_s: torch.Tensor, b: torch.Tensor, b_s: torch.Tensor):\n    \"\"\"\n    Perform a matrix multiplication using FP8 precision.\n    Args:\n        a (torch.Tensor): The first input matrix, must be contiguous.\n        a_s (torch.Tensor): The scaling factor for the first input matrix, must be contiguous.\n        b (torch.Tensor): The second input matrix, must be contiguous.\n        b_s (torch.Tensor): The scaling factor for the second input matrix, must be contiguous.\n    Returns:\n        torch.Tensor: The result of the matrix multiplication.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "fp8_gemm_configs",
        "kind": 5,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "peekOfCode": "fp8_gemm_configs = [\n    Config({'BLOCK_SIZE_M': block_m, 'BLOCK_SIZE_N': block_n, 'BLOCK_SIZE_K': 128}, num_stages=num_stages, num_warps=8)\n    for block_m in [16, 32, 64] for block_n in [32, 64, 128] for num_stages in [3, 4, 5, 6]\n]\n@triton.autotune(configs=fp8_gemm_configs, key=['N', 'K'])\n@triton.jit\ndef fp8_gemm_kernel(a_ptr, b_ptr, c_ptr,\n                    a_s_ptr, b_s_ptr,\n                    M, N: tl.constexpr, K: tl.constexpr,\n                    BLOCK_SIZE_M: tl.constexpr,",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.kernel",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:\n        max_batch_size (int): Maximum batch size.\n        max_seq_len (int): Maximum sequence length.\n        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n        vocab_size (int): Vocabulary size.\n        dim (int): Model dimension.\n        inter_dim (int): Intermediate dimension for MLP layers.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "ParallelEmbedding",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class ParallelEmbedding(nn.Module):\n    \"\"\"\n    Embedding layer with parallelism support across distributed processes.\n    Args:\n        vocab_size (int): Vocabulary size.\n        dim (int): Embedding dimension.\n    \"\"\"\n    def __init__(self, vocab_size: int, dim: int):\n        super().__init__()\n        self.vocab_size = vocab_size",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Linear",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class Linear(nn.Module):\n    \"\"\"\n    Custom linear layer with support for quantized weights and optional bias.\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    dtype = torch.bfloat16",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "ColumnParallelLinear",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class ColumnParallelLinear(Linear):\n    \"\"\"\n    Linear layer with column parallelism, splitting output features across distributed processes.\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Total number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "RowParallelLinear",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class RowParallelLinear(Linear):\n    \"\"\"\n    Linear layer with row parallelism, splitting input features across distributed processes.\n    Args:\n        in_features (int): Total number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n    Args:\n        dim (int): Dimension of the input tensor.\n        eps (float): Epsilon value for numerical stability. Defaults to 1e-6.\n    \"\"\"\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.dim = dim",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "MLA",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class MLA(nn.Module):\n    \"\"\"\n    Multi-Headed Attention Layer (MLA).\n    Attributes:\n        dim (int): Dimensionality of the input features.\n        n_heads (int): Number of attention heads.\n        n_local_heads (int): Number of local attention heads for distributed systems.\n        q_lora_rank (int): Rank for low-rank query projection.\n        kv_lora_rank (int): Rank for low-rank key/value projection.\n        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Gate",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class Gate(nn.Module):\n    \"\"\"\n    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n    Attributes:\n        dim (int): Dimensionality of input features.\n        topk (int): Number of top experts activated for each input.\n        n_groups (int): Number of groups for routing.\n        topk_groups (int): Number of groups to route inputs to.\n        score_func (str): Scoring function ('softmax' or 'sigmoid').\n        route_scale (float): Scaling factor for routing weights.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Expert",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class Expert(nn.Module):\n    \"\"\"\n    Expert layer for Mixture-of-Experts (MoE) models.\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "MoE",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class MoE(nn.Module):\n    \"\"\"\n    Mixture-of-Experts (MoE) module.\n    Attributes:\n        dim (int): Dimensionality of input features.\n        n_routed_experts (int): Total number of experts in the model.\n        n_local_experts (int): Number of experts handled locally in distributed systems.\n        n_activated_experts (int): Number of experts activated for each input.\n        gate (nn.Module): Gating mechanism to route inputs to experts.\n        experts (nn.ModuleList): List of expert modules.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class Block(nn.Module):\n    \"\"\"\n    Transformer block combining attention and feed-forward layers.\n    Attributes:\n        attn (nn.Module): Attention layer (MLA).\n        ffn (nn.Module): Feed-forward network (MLP or MoE).\n        attn_norm (nn.Module): Layer normalization for attention.\n        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n    \"\"\"\n    def __init__(self, layer_id: int, args: ModelArgs):",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer model with positional embeddings, multiple layers, and output projection.\n    Attributes:\n        max_seq_len (int): Maximum sequence length for the transformer.\n        embed (nn.Module): Embedding layer for input tokens.\n        layers (torch.nn.ModuleList): List of transformer blocks.\n        norm (nn.Module): Layer normalization applied after all blocks.\n        head (nn.Module): Output projection layer mapping to vocabulary size.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "linear",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"\n    Applies a linear transformation to the incoming data: y = xA^T + b.\n    This function supports specialized implementations based on quantization\n    and tensor formats.\n    Args:\n        x (torch.Tensor): The input tensor.\n        weight (torch.Tensor): The weight tensor. It may be quantized and \n            requires dequantization for certain cases.\n        bias (Optional[torch.Tensor]): The bias tensor to be added. Default is None.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "precompute_freqs_cis",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n    \"\"\"\n    Precomputes frequency-based complex exponential values for rotary positional embeddings.\n    Args:\n        args (ModelArgs): Model arguments containing positional embedding parameters.\n    Returns:\n        torch.Tensor: Precomputed complex exponential values for positional embeddings.\n    \"\"\"\n    dim = args.qk_rope_head_dim\n    seqlen = args.max_seq_len",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "apply_rotary_emb",
        "kind": 2,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Applies rotary positional embeddings to the input tensor.\n    Args:\n        x (torch.Tensor): Input tensor with positional embeddings to be applied.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n    Returns:\n        torch.Tensor: Tensor with rotary embeddings applied.\n    \"\"\"\n    dtype = x.dtype",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "world_size",
        "kind": 5,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "world_size = 1\nrank = 0\nblock_size = 128\ngemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n@dataclass\nclass ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "rank",
        "kind": 5,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "rank = 0\nblock_size = 128\ngemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n@dataclass\nclass ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:\n        max_batch_size (int): Maximum batch size.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "block_size",
        "kind": 5,
        "importPath": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "description": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "peekOfCode": "block_size = 128\ngemm_impl: Literal[\"bf16\", \"fp8\"] = \"bf16\"\nattn_impl: Literal[\"naive\", \"absorb\"] = \"absorb\"\n@dataclass\nclass ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n    Attributes:\n        max_batch_size (int): Maximum batch size.\n        max_seq_len (int): Maximum sequence length.",
        "detail": "DeepSeek Architecture [LEARNING].DeepSeek-V3.inference.model",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "model_path = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = 50256\ninput_text = \"fortell meg en vits\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\nattention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = 50256\ninput_text = \"fortell meg en vits\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\nattention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "model = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = 50256\ninput_text = \"fortell meg en vits\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\nattention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "model.config.pad_token_id",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "model.config.pad_token_id = 50256\ninput_text = \"fortell meg en vits\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\nattention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "input_text",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "input_text = \"fortell meg en vits\"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\nattention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "input_ids",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "input_ids = tokenizer.encode(input_text, return_tensors='pt')\nattention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "attention_mask",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\noutput = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "output = model.generate(input_ids, attention_mask=attention_mask, min_length=2,max_length=10)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "response",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "peekOfCode": "response = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Test_Inferens_performence_response",
        "documentation": {}
    },
    {
        "label": "read_Trainingdata",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "peekOfCode": "def read_Trainingdata():\n    global training_losses_object, eval_loss_object, epochs  \n    # Path to the file\n    file_path = os.path.join(r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data\\Diagram_eval_trainloss.txt')\n    if os.path.exists(file_path):\n        print(f\"File found: {file_path}\")\n        with open(file_path, 'r') as file:\n            lines = file.readlines()\n            for line in lines:\n                if \"Loss:\" in line:",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "documentation": {}
    },
    {
        "label": "Training_Evolve_Diagram",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "peekOfCode": "def Training_Evolve_Diagram(epochs_return, eval_losses_return, training_losses_return):\n    plt.plot(epochs_return, training_losses_return, label='Trenings Tap', marker='o', linestyle='--', color='blue')\n    plt.plot(epochs_return, eval_losses_return, label='Evaluerings Tap', marker='x', linestyle='-', color='red')\n    OverFitting = any(eval_loss > train_loss for eval_loss, train_loss in zip(eval_losses_return, training_losses_return))\n    if OverFitting:\n            max_epoch = epochs_return[eval_losses_return.index(max(eval_losses_return))] #markerer epoken med høyest eval tap.\n            plt.text(max_epoch, max(eval_losses_return),\"Fitting oppdaget! [tren på mer varierende data], \", color='red',fontsize=12, fontweight='bold')\n    plt.title('Trenings og Evaluerings Tap over Epoker')\n    plt.xlabel('Epoker')\n    plt.ylabel('Tap')",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "documentation": {}
    },
    {
        "label": "training_losses_object",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "peekOfCode": "training_losses_object = []\neval_loss_object = []\nepochs = []\ndef read_Trainingdata():\n    global training_losses_object, eval_loss_object, epochs  \n    # Path to the file\n    file_path = os.path.join(r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data\\Diagram_eval_trainloss.txt')\n    if os.path.exists(file_path):\n        print(f\"File found: {file_path}\")\n        with open(file_path, 'r') as file:",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "documentation": {}
    },
    {
        "label": "eval_loss_object",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "peekOfCode": "eval_loss_object = []\nepochs = []\ndef read_Trainingdata():\n    global training_losses_object, eval_loss_object, epochs  \n    # Path to the file\n    file_path = os.path.join(r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data\\Diagram_eval_trainloss.txt')\n    if os.path.exists(file_path):\n        print(f\"File found: {file_path}\")\n        with open(file_path, 'r') as file:\n            lines = file.readlines()",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "peekOfCode": "epochs = []\ndef read_Trainingdata():\n    global training_losses_object, eval_loss_object, epochs  \n    # Path to the file\n    file_path = os.path.join(r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data\\Diagram_eval_trainloss.txt')\n    if os.path.exists(file_path):\n        print(f\"File found: {file_path}\")\n        with open(file_path, 'r') as file:\n            lines = file.readlines()\n            for line in lines:",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Structure.Model_performence.Trening_graf",
        "documentation": {}
    },
    {
        "label": "CustomCallback",
        "kind": 6,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "class CustomCallback(TrainerCallback): \n    def __init__(self):\n        self.train_loss_list = []\n    def on_log(self, args, state, control, logs=None, **kwargs):\n            if 'loss' in logs:\n                train_loss_list.append(logs['loss'])\n                print(f\"Logging step: {state.global_step}, Loss: {logs['loss']}\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "concatenate_dialogues",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "def concatenate_dialogues(examples):\n    return {\"text\": [\" \".join(dialogue) for dialogue in examples['dialog']]}\ndialogue_dataset = dataset.map(concatenate_dialogues, batched=True, remove_columns=['dialog'])\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\ntokenized_datasets = dialogue_dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nsmall_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in range(2000)])  # 1000 eksempler\nsmall_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # 200 eksempler\nprint(tokenized_datasets['train'][0])",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "tokenize_function",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "def tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\ntokenized_datasets = dialogue_dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nsmall_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in range(2000)])  # 1000 eksempler\nsmall_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # 200 eksempler\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nlog_dir = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_logs\"",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "Write_TrainingData",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "def Write_TrainingData(avg_train_loss, eval_loss_list, epochs_list): \n    data_dir = r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data'\n    os.makedirs(data_dir, exist_ok=True)\n    with open(os.path.join(data_dir, 'Diagram_eval_trainloss.txt'), 'w') as file:\n        train_loss_str = \", \".join(map(str,avg_train_loss)) \n        eval_loss_str = \", \".join(map(str, eval_loss_list))  \n        Epoch_str = \", \".join(map(str, epochs_list)) \n        lines = [f\"Loss: {train_loss_str}\\n\", f\"Eval-loss: {eval_loss_str}\\n\", f\"epochs: {Epoch_str}\\n\"]\n        file.writelines(lines)\ndef adding_loss(loss, eval_loss_value, epoch):",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "adding_loss",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "def adding_loss(loss, eval_loss_value, epoch):\n    train_loss_list.append(loss)\n    eval_loss_list.append(eval_loss_value)\n    epochs_list.append(epoch)\ndef save_checkpoint(trainer, checkpoint_num, train_loss, eval_loss):\n    checkpoint_dir = os.path.join(training_args.output_dir, f\"checkpoint-{checkpoint_num}\")  \n    trainer.save_model(checkpoint_dir)\n    tokenizer.save_pretrained(checkpoint_dir)\n    print(f\"Saved checkpoint to {checkpoint_dir}\")\n    # Logging training information to the specified file",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "def save_checkpoint(trainer, checkpoint_num, train_loss, eval_loss):\n    checkpoint_dir = os.path.join(training_args.output_dir, f\"checkpoint-{checkpoint_num}\")  \n    trainer.save_model(checkpoint_dir)\n    tokenizer.save_pretrained(checkpoint_dir)\n    print(f\"Saved checkpoint to {checkpoint_dir}\")\n    # Logging training information to the specified file\n    with open(log_file_path, \"a\") as log_file:\n        log_file.write(f\"checkpoint: {checkpoint_num}, Train Loss: {train_loss}, Eval Loss: {eval_loss}, epoch: {epoch}\\n\")\n        print(\"Logpoint has been logged!\")\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "model_path = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\ndataset = load_dataset(\"daily_dialog\")\ndataset = dataset['train'].train_test_split(test_size=0.5)  \nprint(dataset)\ndef concatenate_dialogues(examples):\n    return {\"text\": [\" \".join(dialogue) for dialogue in examples['dialog']]}",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\ndataset = load_dataset(\"daily_dialog\")\ndataset = dataset['train'].train_test_split(test_size=0.5)  \nprint(dataset)\ndef concatenate_dialogues(examples):\n    return {\"text\": [\" \".join(dialogue) for dialogue in examples['dialog']]}\ndialogue_dataset = dataset.map(concatenate_dialogues, batched=True, remove_columns=['dialog'])",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "model = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel.resize_token_embeddings(len(tokenizer))\ndataset = load_dataset(\"daily_dialog\")\ndataset = dataset['train'].train_test_split(test_size=0.5)  \nprint(dataset)\ndef concatenate_dialogues(examples):\n    return {\"text\": [\" \".join(dialogue) for dialogue in examples['dialog']]}\ndialogue_dataset = dataset.map(concatenate_dialogues, batched=True, remove_columns=['dialog'])\ndef tokenize_function(examples):",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "dataset = load_dataset(\"daily_dialog\")\ndataset = dataset['train'].train_test_split(test_size=0.5)  \nprint(dataset)\ndef concatenate_dialogues(examples):\n    return {\"text\": [\" \".join(dialogue) for dialogue in examples['dialog']]}\ndialogue_dataset = dataset.map(concatenate_dialogues, batched=True, remove_columns=['dialog'])\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\ntokenized_datasets = dialogue_dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "dataset = dataset['train'].train_test_split(test_size=0.5)  \nprint(dataset)\ndef concatenate_dialogues(examples):\n    return {\"text\": [\" \".join(dialogue) for dialogue in examples['dialog']]}\ndialogue_dataset = dataset.map(concatenate_dialogues, batched=True, remove_columns=['dialog'])\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\ntokenized_datasets = dialogue_dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nsmall_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in range(2000)])  # 1000 eksempler",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "dialogue_dataset",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "dialogue_dataset = dataset.map(concatenate_dialogues, batched=True, remove_columns=['dialog'])\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\ntokenized_datasets = dialogue_dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nsmall_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in range(2000)])  # 1000 eksempler\nsmall_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # 200 eksempler\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "tokenized_datasets",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "tokenized_datasets = dialogue_dataset.map(tokenize_function, batched=True)\ntokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\nsmall_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in range(2000)])  # 1000 eksempler\nsmall_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # 200 eksempler\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nlog_dir = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_logs\"\nos.makedirs(log_dir, exist_ok=True)  \nlog_file_path = os.path.join(log_dir, \"training_log.txt\")",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "small_train_dataset",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select([i for i in range(2000)])  # 1000 eksempler\nsmall_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # 200 eksempler\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nlog_dir = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_logs\"\nos.makedirs(log_dir, exist_ok=True)  \nlog_file_path = os.path.join(log_dir, \"training_log.txt\")\ntraining_args = TrainingArguments(\n    output_dir=model_path,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "small_eval_dataset",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "small_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select([i for i in range(500)])  # 200 eksempler\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nprint(tokenized_datasets['train'][0])\nlog_dir = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_logs\"\nos.makedirs(log_dir, exist_ok=True)  \nlog_file_path = os.path.join(log_dir, \"training_log.txt\")\ntraining_args = TrainingArguments(\n    output_dir=model_path,\n    eval_strategy='epoch',",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "log_dir",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "log_dir = r\"C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_logs\"\nos.makedirs(log_dir, exist_ok=True)  \nlog_file_path = os.path.join(log_dir, \"training_log.txt\")\ntraining_args = TrainingArguments(\n    output_dir=model_path,\n    eval_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.02,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "log_file_path",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "log_file_path = os.path.join(log_dir, \"training_log.txt\")\ntraining_args = TrainingArguments(\n    output_dir=model_path,\n    eval_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.02,\n    logging_dir=log_dir,\n    logging_steps=2,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "training_args",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "training_args = TrainingArguments(\n    output_dir=model_path,\n    eval_strategy='epoch',\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.02,\n    logging_dir=log_dir,\n    logging_steps=2,\n    logging_first_step=True,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "train_loss_list",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "train_loss_list = []\neval_loss_list = []\nepochs_list = []\navg_train_loss = []\ntrainloss = []\ndef Write_TrainingData(avg_train_loss, eval_loss_list, epochs_list): \n    data_dir = r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data'\n    os.makedirs(data_dir, exist_ok=True)\n    with open(os.path.join(data_dir, 'Diagram_eval_trainloss.txt'), 'w') as file:\n        train_loss_str = \", \".join(map(str,avg_train_loss)) ",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "eval_loss_list",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "eval_loss_list = []\nepochs_list = []\navg_train_loss = []\ntrainloss = []\ndef Write_TrainingData(avg_train_loss, eval_loss_list, epochs_list): \n    data_dir = r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data'\n    os.makedirs(data_dir, exist_ok=True)\n    with open(os.path.join(data_dir, 'Diagram_eval_trainloss.txt'), 'w') as file:\n        train_loss_str = \", \".join(map(str,avg_train_loss)) \n        eval_loss_str = \", \".join(map(str, eval_loss_list))  ",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "epochs_list",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "epochs_list = []\navg_train_loss = []\ntrainloss = []\ndef Write_TrainingData(avg_train_loss, eval_loss_list, epochs_list): \n    data_dir = r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data'\n    os.makedirs(data_dir, exist_ok=True)\n    with open(os.path.join(data_dir, 'Diagram_eval_trainloss.txt'), 'w') as file:\n        train_loss_str = \", \".join(map(str,avg_train_loss)) \n        eval_loss_str = \", \".join(map(str, eval_loss_list))  \n        Epoch_str = \", \".join(map(str, epochs_list)) ",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "avg_train_loss",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "avg_train_loss = []\ntrainloss = []\ndef Write_TrainingData(avg_train_loss, eval_loss_list, epochs_list): \n    data_dir = r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data'\n    os.makedirs(data_dir, exist_ok=True)\n    with open(os.path.join(data_dir, 'Diagram_eval_trainloss.txt'), 'w') as file:\n        train_loss_str = \", \".join(map(str,avg_train_loss)) \n        eval_loss_str = \", \".join(map(str, eval_loss_list))  \n        Epoch_str = \", \".join(map(str, epochs_list)) \n        lines = [f\"Loss: {train_loss_str}\\n\", f\"Eval-loss: {eval_loss_str}\\n\", f\"epochs: {Epoch_str}\\n\"]",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "trainloss",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "trainloss = []\ndef Write_TrainingData(avg_train_loss, eval_loss_list, epochs_list): \n    data_dir = r'C:\\Users\\didri\\Desktop\\kopi av learnreflect\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\Model_Training\\Training_data'\n    os.makedirs(data_dir, exist_ok=True)\n    with open(os.path.join(data_dir, 'Diagram_eval_trainloss.txt'), 'w') as file:\n        train_loss_str = \", \".join(map(str,avg_train_loss)) \n        eval_loss_str = \", \".join(map(str, eval_loss_list))  \n        Epoch_str = \", \".join(map(str, epochs_list)) \n        lines = [f\"Loss: {train_loss_str}\\n\", f\"Eval-loss: {eval_loss_str}\\n\", f\"epochs: {Epoch_str}\\n\"]\n        file.writelines(lines)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "data_collator",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\nclass CustomCallback(TrainerCallback): \n    def __init__(self):\n        self.train_loss_list = []\n    def on_log(self, args, state, control, logs=None, **kwargs):\n            if 'loss' in logs:\n                train_loss_list.append(logs['loss'])\n                print(f\"Logging step: {state.global_step}, Loss: {logs['loss']}\")\ntrainer = Trainer(\n    model=model,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "trainer = Trainer(\n    model=model,\n    args=training_args,\n    #train_dataset=tokenized_datasets['train'],\n    #eval_dataset=tokenized_datasets['test'],\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    data_collator=data_collator,\n    callbacks=[CustomCallback()]\n)",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "final_model_path",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "peekOfCode": "final_model_path = os.path.join(training_args.output_dir, f\"final_model_epoch_{training_args.num_train_epochs}\")\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\nprint(f\"Final model and tokenizer saved to {final_model_path}\")\ntorch.cuda.empty_cache()",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Model_Training.Train_Model",
        "documentation": {}
    },
    {
        "label": "CustomDataset",
        "kind": 6,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    # .clone() lager kopi av dataen for å unngå å endre den orginale dataen når man jobber med den.\n    # .detach() dette kobler fra tensoren fra den beregnende grafen som pytorch bruker for å spore gradientene, dette er viktig for å unngå å spore gradienter eller feilaktig oppdatere modellen når vi ikke trener den akk nå.\n    #  'input_ids': den spesifikke setningen med token-IDs som hentes med indeksen idx,\n    #'attention_mask': tilhørende oppmerksomhetsmaske for denne setningen,\n    # ved og bruke clone().detach() sørger vi for å få en sikker kopi av verdiene uten at det påvirkes av andre operasjoner eller treningsberegninger.\n    # self.encodings er som en bok med sider, der hver side har forskjellige kapitler (f.eks. input_ids og attention_mask).\n    # Når du sier val[idx], så henter du én spesifikk side i boken (basert på idx), f.eks. side 10.",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "save_feedback",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "def save_feedback(response_text, feedback_score):\n    with open(\"feedback_data.txt\", \"a\") as f:\n        f.write(f\"{response_text}\\t{feedback_score}\\n\")\n    logging.info(\n        \"Feedback saved: Response: %s, Score: %d\", response_text, feedback_score\n    )\n# Update model immediately with positive feedback\ndef update_model_immediately(response_text, feedback_score):\n    logging.info(\"Starter modell oppdatering med ny respons fra bruker: tekst: %s  score: %d\",response_text,feedback_score,)\n    # Når modellen får tilbakemelding fra frontend, blir responsen som er en tekststreng [response_text] tokenisert og omgjort til tallsekvenser som modellen kan forstå.",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "update_model_immediately",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "def update_model_immediately(response_text, feedback_score):\n    logging.info(\"Starter modell oppdatering med ny respons fra bruker: tekst: %s  score: %d\",response_text,feedback_score,)\n    # Når modellen får tilbakemelding fra frontend, blir responsen som er en tekststreng [response_text] tokenisert og omgjort til tallsekvenser som modellen kan forstå.\n    # Response_text blir behandlet av tokenizer som gjør om teksten til input IDs (tallsekvenser) og \"attention mask\". det returnerer et objekt som innholder nødvending informasjon for modellen å jobbe med.\n    # attention mask: ekte tokens vil få verdien 1, mens padding-tokens får verdien 0, dette gjør at modellen kan ignorere padding-tokens og fokusere kun på de relevante delene av inputen.\n    encodings = tokenizer(\n        [response_text],\n        truncation=True,  # hvis response_text overstiger den angitte max_length på 128 tokens skal den bli kuttet (truncert) altså det overskytende bli fjernet. dette for å unngå feilmeldinger knyttet til inputsstørrelsen i modellen.\n        padding=True,  # Fyller ut sekvenser med spesielle padding-tokens slik at alle sekvenser i en batch har lik lengde. Dette er viktig for effektiv batch-trening. For eksempel, hvis response_text er 100 tokens, vil padding-token legge til 28 tokens i starten eller slutten av sekvensen.\n        max_length=128,  # setter den maksimale lengden på tokensekvensen til 128 tokens. vis tokens overstiges vil det kuttes, dette er nyttig for å unngå lange input sekvenser modellen kanskje ikke kan håndtere.",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "chat",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "def chat():\n    data = (\n        request.json\n    )  # Henter JSON-data fra frontend (innholder meldingen fra bruker (response_text))\n    input_text = data.get(\"message\", \"\")  # henter tekstmelding fra brukeren\n    if (\n        not input_text\n    ):  # hvis meldingen er tom logges en feilelding og returnerer en feilrespons.\n        logging.error(\"Empty message received.\")\n        return jsonify({\"response\": \"Error: No input text provided.\"}), 400",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "feedback",
        "kind": 2,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "def feedback():\n    data = request.json\n    response_text = data.get(\"response\", \"\")\n    feedback_score = int(data.get(\"score\", 0))\n    logging.info(\"Chatbotens respons fra (feedback) i json %s\", response_text)\n    logging.info(\"Feedback numerical score: %d\", feedback_score)\n    if not response_text or feedback_score not in [-1, 1]:\n        logging.error( \"Invalid feedback data: Response: %s, Score: %d\",response_text,  feedback_score,  )\n        return jsonify({\"status\": \"error\", \"message\": \"Invalid feedback data\"}), 400\n    save_feedback(response_text, feedback_score)  # Save feedback",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "chatbot",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "chatbot = Blueprint(\"chatbot\", __name__)\n# laster opp modell og tokenizer\nmodel_path = r\"C:\\Users\\didri\\Documents\\GitHub\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\OriginalModel-Chatbot\\Model\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n# Legg til en spesifikk padding token\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Legger til '[PAD]' som padding token\n# Sett pad_token_id til ID-en av '[PAD]'\nmodel.resize_token_embeddings(len(tokenizer))  # Juster modellens innbilde for det nye tokenet\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "model_path = r\"C:\\Users\\didri\\Documents\\GitHub\\LearnReflect-System\\Python-Backend-Flask\\ChatbotAI\\OriginalModel-Chatbot\\Model\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n# Legg til en spesifikk padding token\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Legger til '[PAD]' som padding token\n# Sett pad_token_id til ID-en av '[PAD]'\nmodel.resize_token_embeddings(len(tokenizer))  # Juster modellens innbilde for det nye tokenet\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n# Definerer et custom dataset\n# Torch.utils.data.dataset er en pytorch klasse som tillater at man lager et dataset som kan bli brukt for trening og evaluering.  ",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n# Legg til en spesifikk padding token\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Legger til '[PAD]' som padding token\n# Sett pad_token_id til ID-en av '[PAD]'\nmodel.resize_token_embeddings(len(tokenizer))  # Juster modellens innbilde for det nye tokenet\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n# Definerer et custom dataset\n# Torch.utils.data.dataset er en pytorch klasse som tillater at man lager et dataset som kan bli brukt for trening og evaluering.  \nclass CustomDataset(torch.utils.data.Dataset):",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "model = GPT2LMHeadModel.from_pretrained(model_path)\n# Legg til en spesifikk padding token\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Legger til '[PAD]' som padding token\n# Sett pad_token_id til ID-en av '[PAD]'\nmodel.resize_token_embeddings(len(tokenizer))  # Juster modellens innbilde for det nye tokenet\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n# Definerer et custom dataset\n# Torch.utils.data.dataset er en pytorch klasse som tillater at man lager et dataset som kan bli brukt for trening og evaluering.  \nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "tokenizer.pad_token_id",
        "kind": 5,
        "importPath": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "description": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "peekOfCode": "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n# Definerer et custom dataset\n# Torch.utils.data.dataset er en pytorch klasse som tillater at man lager et dataset som kan bli brukt for trening og evaluering.  \nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n    # .clone() lager kopi av dataen for å unngå å endre den orginale dataen når man jobber med den.\n    # .detach() dette kobler fra tensoren fra den beregnende grafen som pytorch bruker for å spore gradientene, dette er viktig for å unngå å spore gradienter eller feilaktig oppdatere modellen når vi ikke trener den akk nå.\n    #  'input_ids': den spesifikke setningen med token-IDs som hentes med indeksen idx,\n    #'attention_mask': tilhørende oppmerksomhetsmaske for denne setningen,",
        "detail": "LearnReflect_Languge_Model_ChatbotAI.Route.Route",
        "documentation": {}
    },
    {
        "label": "LearnrReflectMConfig",
        "kind": 6,
        "importPath": "TESTING_LEARNING_FOLDER.Dream_model",
        "description": "TESTING_LEARNING_FOLDER.Dream_model",
        "peekOfCode": "class LearnrReflectMConfig(PretrainedConfig):\n    model_type = \"learnr_reflect_m\"\n    def __init__(\n        self,\n        vocab_size=64000,\n        max_position_embeddings=32768,\n        hidden_size=8192,\n        num_hidden_layers=60,\n        num_attention_heads=64,\n        intermediate_size=32768,",
        "detail": "TESTING_LEARNING_FOLDER.Dream_model",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Dream_model",
        "description": "TESTING_LEARNING_FOLDER.Dream_model",
        "peekOfCode": "config = LearnrReflectMConfig()\n# Use a compatible model class and pass the configuration\nmodel = GPTNeoXForCausalLM(config)\nsave_directory = r\"C:\\Users\\didri\\Desktop\\LearnReflect Project\\LearnReflect-System\\LearnReflect_AI_chatbot\\LearnReflect_Languge_Model_ChatbotAI\\Model_Structure\"\nmodel.save_pretrained(save_directory)\n# Calculate parameter count\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"GPT-4+ Hypothetical Parameter Count: ~{param_count/1e9:.2f}B\")",
        "detail": "TESTING_LEARNING_FOLDER.Dream_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Dream_model",
        "description": "TESTING_LEARNING_FOLDER.Dream_model",
        "peekOfCode": "model = GPTNeoXForCausalLM(config)\nsave_directory = r\"C:\\Users\\didri\\Desktop\\LearnReflect Project\\LearnReflect-System\\LearnReflect_AI_chatbot\\LearnReflect_Languge_Model_ChatbotAI\\Model_Structure\"\nmodel.save_pretrained(save_directory)\n# Calculate parameter count\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"GPT-4+ Hypothetical Parameter Count: ~{param_count/1e9:.2f}B\")",
        "detail": "TESTING_LEARNING_FOLDER.Dream_model",
        "documentation": {}
    },
    {
        "label": "save_directory",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Dream_model",
        "description": "TESTING_LEARNING_FOLDER.Dream_model",
        "peekOfCode": "save_directory = r\"C:\\Users\\didri\\Desktop\\LearnReflect Project\\LearnReflect-System\\LearnReflect_AI_chatbot\\LearnReflect_Languge_Model_ChatbotAI\\Model_Structure\"\nmodel.save_pretrained(save_directory)\n# Calculate parameter count\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"GPT-4+ Hypothetical Parameter Count: ~{param_count/1e9:.2f}B\")",
        "detail": "TESTING_LEARNING_FOLDER.Dream_model",
        "documentation": {}
    },
    {
        "label": "param_count",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Dream_model",
        "description": "TESTING_LEARNING_FOLDER.Dream_model",
        "peekOfCode": "param_count = sum(p.numel() for p in model.parameters())\nprint(f\"GPT-4+ Hypothetical Parameter Count: ~{param_count/1e9:.2f}B\")",
        "detail": "TESTING_LEARNING_FOLDER.Dream_model",
        "documentation": {}
    },
    {
        "label": "LearnrReflectMini",
        "kind": 6,
        "importPath": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "description": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "peekOfCode": "class LearnrReflectMini(PretrainedConfig):\n    model_type = \"LearnrReflectMini\"\n    def __init__(\n        self,\n        vocab_size=32000,\n        max_position_embeddings=2048,\n        hidden_size=768,         # Much smaller\n        num_hidden_layers=12,    # Reduced layer count\n        num_attention_heads=12,  # Must divide hidden_size evenly\n        intermediate_size=3072,  # 4 * hidden_size",
        "detail": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "documentation": {}
    },
    {
        "label": "#Heads",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "description": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "peekOfCode": "#Heads = 12: Divides 768 evenly (768 / 12 = 64).\n#Vocab size = 32000: You could use 32000 or 64000—lower is often enough for a small test.\n#Max position embeddings = 2048: Enough to test chunked training or attention over a decent context, while not being too big.\n#Use rotary: You can still keep rotary_emb_fraction=1.0 to confirm the code path for rotary embeddings is fine.\n#In practice, this small config should land around 100M–150M parameters (depending on exact final shapes) and let you:\n#Tokenize a dataset.\n#Load batches and verify memory usage / speed.\n#Train for a few steps to confirm loss is decreasing.\n#Debug any pipeline issues quickly.\n#Once you’ve validated that the pipeline is stable, you can scale back up to your “dream” 70‑layer, 10240‑hidden LearnrReflectM+ monster!",
        "detail": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "description": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "peekOfCode": "config = LearnrReflectMini()\nmodel = GPTNeoXForCausalLM(config)\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"LearnrReflectMini Parameter Count: ~{param_count/1e6:.1f}M\")",
        "detail": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "description": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "peekOfCode": "model = GPTNeoXForCausalLM(config)\nparam_count = sum(p.numel() for p in model.parameters())\nprint(f\"LearnrReflectMini Parameter Count: ~{param_count/1e6:.1f}M\")",
        "detail": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "documentation": {}
    },
    {
        "label": "param_count",
        "kind": 5,
        "importPath": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "description": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "peekOfCode": "param_count = sum(p.numel() for p in model.parameters())\nprint(f\"LearnrReflectMini Parameter Count: ~{param_count/1e6:.1f}M\")",
        "detail": "TESTING_LEARNING_FOLDER.Pipeline_model",
        "documentation": {}
    }
]
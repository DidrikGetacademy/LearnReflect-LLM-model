import torch
import time
import os
import psutil
from LR_AI_LLM.Model_Structure.Model_Class import LearnrReflectM, LearnrReflectMConfig  # Import your model

# âœ… Ensure PyTorch runs on GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster training

### ðŸ”¹ Function to Check VRAM & CPU RAM Usage ###
def check_memory():
    vram = torch.cuda.memory_allocated() / (1024 ** 3) if torch.cuda.is_available() else 0
    ram = psutil.virtual_memory().used / (1024 ** 3)
    return f"ðŸ”¹ VRAM Usage: {vram:.2f} GB | ðŸ”¹ RAM Usage: {ram:.2f} GB"

# âœ… Print initial memory state
print("ðŸ”¹ Initial Memory State:", check_memory())

# âœ… Load Model Configuration
config = LearnrReflectMConfig()  # Uses optimized config for single GPU

# âœ… Load Model & Move to GPU
model = LearnrReflectM(config).to(device)
model = torch.compile(model)  # ðŸ”¹ Use `torch.compile()` for speed optimization
model = model.to(dtype=torch.float16)  # ðŸ”¹ Use mixed precision for lower VRAM

# âœ… Create dummy input for testing (batch=1, seq_len=512)
batch_size = 1
seq_len = 512  # ðŸ”¹ Reduce sequence length for single GPU
x = torch.randint(0, config.vocab_size, (batch_size, seq_len)).to(device)

# âœ… Run Inference & Measure Time
torch.cuda.synchronize()
start_time = time.time()

with torch.no_grad():
    output = model(x)

torch.cuda.synchronize()
end_time = time.time()

# âœ… Print Inference Results
print("ðŸ”¹ Model Output Shape:", output.shape)
print("ðŸ”¹ Inference Time:", f"{end_time - start_time:.3f} sec")
print("ðŸ”¹ Memory After Inference:", check_memory())

# âœ… Save Model Checkpoint (Optional)
torch.save(model.state_dict(), "learnr_reflectm_checkpoint.pth")
print("âœ… Model checkpoint saved!")

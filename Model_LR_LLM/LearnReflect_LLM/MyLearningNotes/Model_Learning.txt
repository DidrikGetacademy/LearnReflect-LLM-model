####CONFIG PARAMETERS######
vocab_size=64000,#vocab_size represents the number of unique tokens (words, subwords, or characters) the model can understand. Larger vocabulary size allows the model to recognize more words and subwords, leading to better understanding of rare words.
max_position_embeddings=32768, #If max_position_embeddings = 32768, the model can handle 32,768 tokens in one input. More tokens = Longer memory â†’ Can understand books, articles, or long dialogues without forgetting previous context.
hidden_size=8192, #Hidden size is the number of neurons in each layer of the Transformer.Larger hidden_size = More information stored in each step. This directly affects the model's ability to represent complex information.More powerful but requires more VRAM (GPU memory).
num_hidden_layers=60, #Each hidden layer is a Transformer block that processes input step-by-step.More layers = Deeper understanding and better ability to generate meaningful responses. Each layer contains [ Multi-Head Attention (MHA) Feedforward Neural Networks (FFN)Layer Normalization]
num_attention_heads=64, #Each attention head processes information independently. More heads = Better ability to focus on different parts of input at the same time. Multi-head attention (MHA) splits information across multiple parallel layers. More heads = More parallel attention = Better understanding of complex relationships. Fewer heads = Less ability to track different concepts at once.Think of attention heads like "brain processes." More heads = More focus areas in parallel.ðŸ”¹ Think of attention heads like "brain processes." More heads = More focus areas in parallel.
intermediate_size=32768, #Defines the size of the Feedforward Neural Network (FFN) inside each Transformer layer. Each Transformer block has two main components:Self-Attention (MHA - Multi-Head Attention)Feedforward Neural Network (FFN) A larger intermediate_size (e.g., 32768) improves the modelâ€™s ability to extract deep hierarchical features. Helps the model generate more accurate, complex responses.Increasing this increases computation and memory usage.
hidden_act="swiGLU",#Defines the activation function used in the Feedforward Network (FFN) inside each Transformer layer. Instead of ReLU or GELU, this model uses "swiGLU" (Swish-Gated Linear Unit).  Improves training efficiency over ReLU.  Faster convergence and better gradient flow.
rotary_emb_fraction=1.0,    # Controls how much of the hidden size is used for Rotary Positional Embeddings (RoPE).1.0 means all attention heads use RoPE.If set to 0.5, only half of the attention heads will use RoPE.
use_flash_attention=True, #Flash Attention is a new, optimized self-attention mechanism,It reduces memory usage and computational cost. It reduces memory usage and computational cost. Speeds up training and inference by up to 3x. Standard attention is slow (O(nÂ²) complexity) â†’ Flash Attention makes it faster (O(n log n)).
layer_norm_eps=1e-6,#Epsilon (eps) is a small constant added to prevent division by zero in Layer Normalization. âœ… Ensures numerical stability in Layer Normalization.âœ… Prevents NaN (not-a-number) errors when normalizing activations.âœ… Typically set between 1e-5 and 1e-12.
rope_theta=10000,  #RoPE (Rotary Positional Embeddings) replaces absolute position embeddings.It preserves distance information between words more effectively. Helps the model understand long-range dependencies better than absolute embeddings. ðŸ”¹ Think of it as a way to encode word positions in a more intelligent way.
attention_bias=True,#Enables learnable bias terms inside the attention layers. Bias terms help the model generalize better. Some LLMs remove attention bias to reduce parameter count (LLaMA-2 uses attention_bias=False).


###AI DEVELOPMENT######
Where Does AI Development Start?
âœ… Neuroscience & Cognitive Science â†’ Inspiration from the brain.
âœ… Programming & Deep Learning â†’ Implementing algorithms using Python, PyTorch, TensorFlow.
âœ… Hardware Optimization â†’ Making AI run faster on GPUs & TPUs.


âœ…People improve AI techniques by experimenting and modifying existing architectures
Example: GPT-4 vs GPT-3
GPT-4 introduced Mixture of Experts (MoE) to make training more efficient.
LLaMA 2 reduced unnecessary bias parameters to save memory.
How They Did It?
Researchers analyzed the inefficiencies in GPT-3.
They created new layers and training strategies to improve GPT-4.



âœ…Developing New Attention Mechanisms
Example: Flash Attention (Used in LLaMA 2, GPT-4)
Instead of using traditional self-attention, researchers optimized it for speed.
This reduced memory bottlenecks in GPUs.
How They Did It?
They modified the Attention class to use a new memory-efficient algorithm.




âœ…Creating New Learning Techniques
Example: Reinforcement Learning from Human Feedback (RLHF)

Used in ChatGPT, Bard, Claude AI to make responses more human-like.
Instead of training just on text, the model learns from human feedback.
How They Did It?

OpenAI trained a reward model that learns what humans prefer.
Then, they used Reinforcement Learning (RL) to fine-tune GPT-4.





âœ…Improving Training Data & Tokenization
Example: Byte Pair Encoding (BPE) vs. SentencePiece

AI models donâ€™t read full words but break them into subwords.
Googleâ€™s T5 model used SentencePiece, which handles multiple languages better.
How They Did It?

They experimented with different tokenization techniques and measured which one worked best.


âœ…Hardware & Parallelism Innovations
Example: DeepSpeed & Tensor Parallelism

Large models (like GPT-4) donâ€™t fit on a single GPU.
AI engineers use Parallel Training to split a model across multiple GPUs.
How They Did It?

They modified PyTorchâ€™s model training functions to efficiently distribute computation.
